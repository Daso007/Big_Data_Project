{"cells": [{"cell_type": "markdown", "id": "60fc2ec5-5160-48e4-83e1-1421a81289fd", "metadata": {}, "source": "# Key Consideration for Your Dataproc Cluster\n\n1. Cluster Resources:mm\n\n.Master: n2-standard-4 (4 vCPUs, 16 GB RAM, 32GB disk)\n.Workers(2x): n2-standard-4 (4 vCPUs, 16 GB RAM , 64 GB disk each)\n.Total: 8 worker vCPUs, ~32 GB RAM (excluding master node)\n\n2. Dataproc Features Disabled:\n\n.No autoscaling, Metastore, advanced execution layer, advanced optimizations\n.Storage: pd-balanced (no SSDs, so I/O optimization is crucial)\n.Networing: Internal IP enabled\n\n3. Optimization Strategy:\n\n.Tune shuffle partitions, broadcast join threshold, and storage persistence\n.Adjust parallelism based on 2 workersx4 cores\n.Avoid execessive caching due to disk-based storage"}, {"cell_type": "code", "execution_count": 1, "id": "a9a4db39-f05e-4dc0-bdfb-56c9f02911c8", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "0d3b4b68-fae2-4c75-8036-d6247b577ad8", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/15 03:32:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder\\\n.appName('Olist Ecommerce Performance Optimization')\\\n.config('spark.executor.memory','6g')\\\n.config('spark.executor.cores','4')\\\n.config('spark.executor.instances','2')\\\n.config('spark.driver.memory','4g')\\\n.config('spark.driver.maxResultSize','2g')\\\n.config('spark.sql.shuffle.partitions','64')\\\n.config('spark.default.parallelism','64')\\\n.config('spark.sql.adaptive.enabled','true')\\\n.config('spark.sql.adaptive.coalescePartition.enabled','true')\\\n.config('spark.sql.autoBroadcatJoinThreshold',20*1024*1024)\\\n.config('spark.sql.files.maxPartitionBytes','64MB')\\\n.config('spark.sql.files.openCostInBytes','2MB')\\\n.config('spark.memory.fraction',0.8)\\\n.config('spark.memory.storageFraction',0.2)\\\n.getOrCreate()"}, {"cell_type": "markdown", "id": "1b682021-a5c8-4e74-bc27-96385dbc0278", "metadata": {}, "source": "\u2699\ufe0f Cluster Summary\n\ud83d\udca1 Hardware Setup\nMaster: n2-standard-4 (4 vCPU, 16 GB RAM)\n\nWorkers (2\u00d7): n2-standard-4 (8 vCPUs total, ~32 GB RAM)\n\nDisk: pd-balanced (HDD-like, not SSD \u2014 so disk I/O is a performance bottleneck)\n\n\ud83d\udd27 Spark Configuration Explained\n\nspark = SparkSession.builder\\\n\n\n\ud83e\udde0 Memory & Executor Settings\n\n.config('spark.executor.memory','6g')\n.config('spark.executor.cores','4')\n.config('spark.executor.instances','2')\nexecutor.memory=6g: Each executor gets 6 GB RAM.\n\nexecutor.cores=4: Each executor uses all 4 cores of a worker node (since you have 2 workers with 4 cores each).\n\nexecutor.instances=2: One executor per worker. This aligns with your cluster (2 workers \u00d7 4 cores).\n\n\u2705 Best practice: Assign one executor per node to fully utilize resources without contention.\n\n\ud83e\udde0 Driver Settings\n\n.config('spark.driver.memory','4g')\n.config('spark.driver.maxResultSize','2g')\ndriver.memory=4g: Driver runs on the master node (16 GB available).\n\nmaxResultSize=2g: Prevents massive result sets from overwhelming the driver.\n\n\ud83d\udd04 Shuffle & Parallelism Tuning\n\n.config('spark.sql.shuffle.partitions','64')\n.config('spark.default.parallelism','64')\nshuffle.partitions=64: Controls number of output partitions after joins/groupBy.\n\ndefault.parallelism=64: Default number of tasks. Good for 8 cores \u00d7 2 tasks/core = 16\u201364 range.\n\n\ud83d\udca1 For your 8-core cluster, 64 is a safe upper bound to avoid under-parallelization.\n\n\u2699\ufe0f Adaptive Query Execution (AQE)\n\n.config('spark.sql.adaptive.enabled','true')\n.config('spark.sql.adaptive.coalescePartition.enabled','true')\nAQE: Dynamically adjusts partition count, avoids skew, optimizes join strategies at runtime.\n\nCoalesce enabled: Reduces small partitions after shuffles \u2192 better performance.\n\n\u2705 This is crucial when you don\u2019t know data sizes in advance.\n\n\ud83d\udd01 Broadcast Join Threshold\n\n.config('spark.sql.autoBroadcatJoinThreshold',20*1024*1024)\n20 MB threshold: Tables smaller than 20 MB will be broadcast in joins, saving expensive shuffles.\n\n\u2705 Useful when joining dimension tables (e.g., customers, sellers) with large fact tables.\n\n\ud83d\udcc2 File I/O Tuning (especially important due to no SSD)\n\n.config('spark.sql.files.maxPartitionBytes','64MB')\n.config('spark.sql.files.openCostInBytes','2MB')\n64 MB partition size: Avoids over-partitioning and excessive file reads.\n\n2 MB open cost: Helps Spark optimize I/O operations by assigning fewer files per task if they are expensive to open.\n\n\u2705 This compensates for your pd-balanced disks (which are slower than SSDs).\n\n\ud83d\udcbe Memory Management\n\n.config('spark..memory.fraction',0.8)       # Typo: extra dot\n.config('spark.memory.storageFraction',0.2)\nmemory.fraction=0.8: 80% of executor memory is used for execution and storage (default is 0.6)\n\nstorageFraction=0.2: Of that, only 20% is for caching. Leaves more room for computation.\n\n\n\ud83e\udde0 Summary of Your Optimization Strategy\nArea\tStrategy & Rationale\nExecutors\tOne per node with full resource use.\nDriver\tConservative size to handle large operations safely.\nShuffle\tBalanced partitioning (64) for better performance on 8-core setup.\nAQE\tDynamic optimization based on runtime data size & skew.\nBroadcast Joins\tEfficient small table joins using threshold.\nStorage Tuning\tOptimal file partition size and reduced open cost to handle slow disk I/O.\nMemory\tFavor execution over caching since you're not using SSDs and caching is expensive."}, {"cell_type": "code", "execution_count": 3, "id": "a0f9c22d-d583-4ab4-90da-0744fadbc077", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order_df = spark.read.parquet('/data/olist_proc/full_order_df_joined.parquet')"}, {"cell_type": "code", "execution_count": 4, "id": "7bdd9588-de96-4361-b1e4-cc70d4538cf0", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = '/data/olist/'"}, {"cell_type": "code", "execution_count": 5, "id": "6b478984-9bf7-40e6-a0b2-0eebcb184c0e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv',header=True,inferSchema=True)\ngeolocation_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv',header=True,inferSchema=True)\norder_items_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv',header=True,inferSchema=True)\npayments_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv',header=True,inferSchema=True)\nreviews_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv',header=True,inferSchema=True)\norders_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv',header=True,inferSchema=True)\nproducts_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv',header=True,inferSchema=True)\nsellers_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv',header=True,inferSchema=True)\ncategory_translation_df = spark.read.csv(hdfs_path + 'product_category_name_translation.csv',header=True,inferSchema=True)"}, {"cell_type": "markdown", "id": "7a5a270e-1ea8-459f-8193-1989d33261b9", "metadata": {}, "source": "# Optimized Join Strategies"}, {"cell_type": "code", "execution_count": 6, "id": "7e873d06-c222-440f-a9a2-53fc36b41dd8", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import *"}, {"cell_type": "code", "execution_count": 7, "id": "e3685688-4513-4ae1-aeb5-378b077928e4", "metadata": {"tags": []}, "outputs": [], "source": "# Broadcast\n\ncustomer_broadcast_df = broadcast(customers_df)\n\noptimized_broadcast_join = full_order_df.join(customer_broadcast_df,'customer_id')"}, {"cell_type": "code", "execution_count": 12, "id": "9f741031-6993-4858-b42e-44f9ed9c19b1", "metadata": {"tags": []}, "outputs": [], "source": "# Sort and Merge join\n\nsorted_customers_df = customers_df.sortWithinPartitions('customer_id')\nsorted_orders_df = full_order_df.sortWithinPartitions('customer_id')\n\noptimized_merge_full_order_df = sorted_orders_df.join(sorted_customers_df,'customer_id')"}, {"cell_type": "code", "execution_count": 13, "id": "1a1919bb-9b50-4155-99c3-772764e44d08", "metadata": {"tags": []}, "outputs": [], "source": "# Bucket Join\n\nbucketed_customers_df = customers_df.repartition(10,'customer_id')\nbucketed_orders_df = full_order_df.repartition(10,'customer_id')\n\nbucketed_join_df = bucketed_orders_df.join(bucketed_customers_df,'customer_id')"}, {"cell_type": "code", "execution_count": 19, "id": "93ae6c2f-ef23-4f3d-82e3-b76545a6ac3f", "metadata": {"tags": []}, "outputs": [], "source": " # Skew join handling\n    \nskew_handled_join = full_order_df.join(customers_df,'customer_id')"}, {"cell_type": "code", "execution_count": 17, "id": "e34870ed-fd4c-43f5-a4ae-b9683e0e855a", "metadata": {"tags": []}, "outputs": [], "source": "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", 64 * 1024 * 1024)  # default\n"}, {"cell_type": "code", "execution_count": 1, "id": "0e056b4c-e408-4a7a-9950-0ca55e3a3807", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "2985fec6-82d6-424e-987c-5471779433ec", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}, "version_major": 2, "version_minor": 0}}}, "nbformat": 4, "nbformat_minor": 5}